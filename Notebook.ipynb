{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505aff0b",
   "metadata": {},
   "source": [
    "# RAG Task - PwC Junior AI Engineer Application Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a9301",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70038c0",
   "metadata": {},
   "source": [
    "This notebook demonstrates my solution at creating an agentic chatbot with autonomous decision making, retrieval-augmented generation (RAG) and integration of external knowledge via FAISS.\n",
    "Although the notebook contains `.py` code snippets, the source code is logically separated into 2 Python files - one responsible for executing the LangGraph-based chatbot (langGraphProt.py), and the other responsible for vectorizing datasets (dataLoader.py). This is so that if new datasets are to be added (or the current one is to be expanded), it can be easily done without messing with the main program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbe5e0",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b9052",
   "metadata": {},
   "source": [
    "Install all required libraries:\n",
    "\n",
    "`pip install faiss-cpu numpy torch sentence-transformers PyPDF2 requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cda66",
   "metadata": {},
   "source": [
    "## Design Decisions\n",
    "- **LLM**: For choosing a Large Language Model, I had to balance reasoning ability, context window size, parameter size, latency, memory requirement and quality of generation, all while ensuring the model is free to use. Since PwC usersâ€™ queries may require summarization, handling long documents, or multi-step reasoning, I needed a model with strong long-text and reasoning performance that remains accessible for prototyping. Initially I chose DeepSeek R1, but after some mixed-quality / inconsistent answers for English questions, I decided to switch model. I chose to work with keys available on [OpenRouter](https://openrouter.ai/). My final choice was my alternative, the [model](https://openrouter.ai/meta-llama/llama-3.3-70b-instruct:free) Meta Llama 70B, because multiple suitable models that I have tried working with were either rate-limited, or were throwing error, saying that my query is flagged unexpectedly. However, this model perfectly meets the requirements I described above, and was suitable for me for prototyping.\n",
    "- **Embedding model**: I have selected an embedding model that was present on the HuggingFace website's [leaderboard](https://huggingface.co/spaces/mteb/leaderboard). I wanted a model with good semantic capture at a reasonable cost (latency and storage). So I choose a model with 0.6B parameter size that is feasible to work with on my laptop, but is still good for prototyping. My choice is the [Qwen3-Embeddings-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) with 595M parameters, 1024-dimension vectors, solid retrieval time, but still a lightweight model. \n",
    "- **FAISS**: I chose [FAISS](https://faiss.ai/index.html) because it is an efficient, lightweight and is a widely used library for similarity search on dense vectors. It allows me to quickly retrieve relevant chunks of texts using vector embeddings. Since scalability is crucial, FAISS is a strong choice because it can handle millions of vectors really quickly and can be easily integrated with Python.\n",
    "- **Pipeline**: I designed a pipeline as a state graph with clearly defined nodes (input -> retriever -> controller -> LLM -> output) to demonstrate agentic behaviour and modularity. While this structure is simple, it efficiently showcases autonomous decisionmaking and keeps the prototype scalable and maintainable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0d30d",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "- **Performance**: I could test the performance of the chatbot in two steps: first is by measuring the speed of embedding the dataset, and the other is by measuring time it takes for individual nodes in the pipeline to execute code. As for the former, vectorization takes about 20 seconds on average, with the nodes in the main program having to execute for differing amounts of time. In most cases during development, the LLM node took the longest to execute, due to having to make an API call and receive a formulated response. Other nodes took about 1 second to execute on average.   \n",
    "\n",
    "- **Bottlenecks**:\n",
    "1. Embedding generation: chunk embeddings are precomputed, but but query embeddings are computed at runtime. We can use a lightweight embedding model for queries to decrease latency, and/or cache repeated queries.\n",
    "2. FAISS search: usually fast, but with large datasets the search time increases and memory usage can spike.\n",
    "3. LLM call: due to network latency + model inference time. Using a large remote model is the slowest part.\n",
    "4. Controller retries: each retry doubles computation, increasing latency\n",
    "5. State printing: JSON dumps can slow down debug output \n",
    "- **Future improvements**:\n",
    "1. Embedding model: Currently, it is lightweight, free, and runs locally. In the future, multilingual embedding models can be used that offer better semantic accuracy, broader language coverage and larger context window. They would run on decentralized servers with GPUs (PwC cloud for example) or specialized vector database services (Pinecone for example).\n",
    "2. LLM: Currently, it is lightweight and free, and runs externally. In the future, multilingual LLMs can be used with stronger reasoning abilities that would run on PwC's secure cloud environment (for compliance) or via private API gateways (so data never leaves PwC infrastructure). \n",
    "3. Retrieval: Currently, the dataset is small and local. In the future, we can increease the RAG dataset (more documents / chunks, and web access), and can increase the retrieval accuracy via better embedding models, more sophisticated chunking, multi-hop retrieval and by adding metadata. \n",
    "4. Pipeline: In the future, we can integrate sources into answers, can include logging or state visualization, and can improve the controller by a smarter coverage thresholding and conditional retries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d351e14",
   "metadata": {},
   "source": [
    "## Source Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3aa049",
   "metadata": {},
   "source": [
    "As said, there are two python files - `dataLoader.py` and `langGraphProt.py`, which can also be found in `src/`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5ec8f",
   "metadata": {},
   "source": [
    "`dataLoader.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "\n",
    "##ADJUSTABLE PARAMETERS##\n",
    "filename = \"1706.03762v7.pdf\"\n",
    "#In this case, it can be replaced with any PDF\n",
    "embeddingModel = SentenceTransformer(\"models/qwen/Qwen3-Embedding-0.6B\")\n",
    "#Load the embedding model\n",
    "chunkS = 500\n",
    "#Preferred chunk size\n",
    "overl = 50\n",
    "#Preferred overlap\n",
    "\n",
    "\n",
    "\n",
    "#Function that turns PDF into text using the PyPDF2 library\n",
    "def turnToText(path):\n",
    "    read = PyPDF2.PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in read.pages:\n",
    "        #Extract page by page\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "#Function that splits the extracted text into chunks\n",
    "def splitter(text):\n",
    "    chunkSize = chunkS\n",
    "    overlap = overl\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunkSize - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunkSize])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = [] \n",
    "#To ensure it will not be ran (causing high latency) during import\n",
    "if __name__ == \"__main__\": \n",
    "    pdfText = turnToText(filename)\n",
    "    chunks = splitter(pdfText)\n",
    "    #Create chunks of text of a given file\n",
    "\n",
    "    chunk_vectors = []\n",
    "    for chunk in chunks:\n",
    "        #Vectorize chunks using the embedding model\n",
    "        vector = embeddingModel.encode(chunk, convert_to_numpy=True)\n",
    "        chunk_vectors.append(vector)\n",
    "    chunk_vectors = np.array(chunk_vectors).astype(\"float32\") \n",
    "    #FAISS needs float32\n",
    "\n",
    "    with open(\"src/faiss_index/chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2) \n",
    "        #Save chunks as texts too\n",
    "\n",
    "    dimensions = chunk_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimensions)\n",
    "    #Grab vector size (dimensions) and index with it\n",
    "    index.add(chunk_vectors)\n",
    "\n",
    "    faiss.write_index(index, \"src/faiss_index/chunk_index.faiss\")\n",
    "    print(\"FAISS saved\")\n",
    "    #Write FAISS to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7b851",
   "metadata": {},
   "source": [
    "`langGraphProt.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import initialized.\")\n",
    "import time\n",
    "startImp = time.perf_counter()\n",
    "import os\n",
    "import requests\n",
    "print(\"Loading FAISS and embedding model, please wait...\")\n",
    "import faiss\n",
    "from dataLoader import embeddingModel, chunks\n",
    "import numpy as np\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "endImp = time.perf_counter()\n",
    "print(f\"Import successful (took {endImp-startImp:.1f} seconds). Initializing graph...\")\n",
    "#At this point, all imports are done (timed for performance)\n",
    "\n",
    "\n",
    "\n",
    "##ADJUSTABLE PARAMETERS##\n",
    "os.environ[\"API_KEY\"] = \"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\"\n",
    "#Replace with your API key\n",
    "SITE_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "#Replace with a preferred site URL\n",
    "langModel = \"meta-llama/llama-3.3-70b-instruct:free\"\n",
    "#Replace with a preferred model\n",
    "kValue = 5\n",
    "#Replace with a preferred amount of chunks (context) to fetch\n",
    "minCoverage = 0.7\n",
    "#Replace with a preferred minimum of keyword coverage in the retrieved chunks\n",
    "maxDistance = 1.2\n",
    "#Replace with a preferred max distance of similarity\n",
    "\n",
    "\n",
    "\n",
    "faissIndexPath = \"src/faiss_index/chunk_index.faiss\"\n",
    "#Contains FAISS index, used for quick and efficient similarity search - created upon running dataLoader.py\n",
    "index = faiss.read_index(faissIndexPath)\n",
    "with open(\"src/faiss_index/chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_texts = json.load(f)\n",
    "    #Read the chunks as text too to pass on to the LLM alongside with the query\n",
    "print(f\"Chunks successfully loaded.\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages] \n",
    "    retrieval: dict\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "#A simple function to retrieve keywords from a query\n",
    "def getKeywords(text):\n",
    "    commonWords = {\"a\", \"an\", \"the\", \"is\", \"and\", \"or\", \"for\", \"to\", \"with\", \"of\", \"on\", \"in\"}\n",
    "    #Words to ignore\n",
    "    allWords = re.findall(r\"\\w+\", text.lower())\n",
    "    keywords = [w for w in allWords if w not in commonWords]\n",
    "    #Retrieve keywords\n",
    "    return keywords\n",
    "#A simple function that returns the percentage of keywords covered in the retrieved chunks\n",
    "def keywordCoverage(query, chunks):\n",
    "    keywords = set(getKeywords(query))\n",
    "    #Retrieve keywords using the above function\n",
    "    chunk_text = \" \".join(chunks).lower()\n",
    "    matched = [kw for kw in keywords if kw in chunk_text]\n",
    "    #Get words that match and return the ratio (percentage of covered keywords)\n",
    "    return len(matched) / (len(keywords) or 1)\n",
    "\n",
    "\n",
    "#The first node in the pipeline\n",
    "def input_node(state: State):\n",
    "    query = input(\"(CTRL+C to quit) Enter your question: \").strip()\n",
    "    state[\"messages\"].append(HumanMessage(content=query))\n",
    "    #Takes the input from the user, appends it into the state corresponding to messages,\n",
    "    #and returns it to the next node: the Retriever\n",
    "    return state\n",
    "graph_builder.add_node(\"input\", input_node)\n",
    "\n",
    "\n",
    "#The second node in the pipeline, responsible for fetching chunks relevant to the user query\n",
    "#From here on, all time-consuming nodes are timed to measure performance\n",
    "def retriever_node(state: State):\n",
    "    start = time.perf_counter()\n",
    "    query = state[\"messages\"][-1].content\n",
    "    #Retrieve user query\n",
    "    query_vector = embeddingModel.encode(query, convert_to_numpy=True).astype(\"float32\")\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    #Vectorize the user query using the embedding model\n",
    "    distances, indices = index.search(query_vector, kValue)\n",
    "    chunks = [chunk_texts[i] for i in indices[0] if i<len(chunk_texts)]\n",
    "    #Retrieve top k similar chunks (similarity determined via distance)\n",
    "    state[\"retrieval\"] = {\n",
    "        \"chunks\": chunks, \"distances\": distances[0].tolist()\n",
    "    }\n",
    "    #Update the state with the chunks\n",
    "    end = time.perf_counter()\n",
    "    print(f\"RETRIEVER node took {end-start:.1f} seconds\")\n",
    "    return state\n",
    "graph_builder.add_node(\"retriever\", retriever_node)\n",
    "\n",
    "\n",
    "#Auxiliary function to reformulate a given query using the selected LLM\n",
    "def reformulateQuery(query):\n",
    "    start = time.perf_counter()\n",
    "    #Construct header of the message\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['API_KEY']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    #Construct payload (chosen model and rewriting task with the given query)\n",
    "    payload = {\n",
    "        \"model\": langModel,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that rewrites user queries into cleaner search queries\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Rewrite this question so it works better for searching documents: {query}\"}\n",
    "        ]\n",
    "    }\n",
    "    #Post the question and obtain response message \n",
    "    resp = requests.post(SITE_URL, json=payload, headers=headers)\n",
    "    result = resp.json()\n",
    "\n",
    "    try:\n",
    "        toResult = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        print(f\"Rewrote question: {toResult}\")\n",
    "        #Extract response (rewrote question)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"REFORMULATOR took {end-start:.1f} seconds\")\n",
    "        return toResult\n",
    "    except:\n",
    "        print(\"Failed to rewrite query\")\n",
    "        #If error, keep original message\n",
    "        end = time.perf_counter()\n",
    "        print(f\"REFORMULATOR took {end-start:.1f} seconds\")\n",
    "        return query\n",
    "\n",
    "\n",
    "#The third node in the pipeline, responsible for deciding if more relevant chunks are required (and fetching them)\n",
    "def controller_node(state: State):\n",
    "    #Simple function to vectorize the query\n",
    "    def encodeQuery(q):\n",
    "        query_v = embeddingModel.encode(q, convert_to_numpy=True).astype(\"float32\")\n",
    "        return np.expand_dims(query_v, axis=0)\n",
    "    start = time.perf_counter()\n",
    "    query = state[\"messages\"][-1].content\n",
    "    chunks = state.get(\"retrieval\", {}).get(\"chunks\", [])\n",
    "    #Extract query and relevant chunks\n",
    "    coverage = keywordCoverage(query, chunks)\n",
    "    print(f\"Keyword coverage: {coverage:.2f}\")\n",
    "    distances = state.get(\"retrieval\", {}).get(\"distances\", [])\n",
    "    #Calculate keyword coverage and distances\n",
    "\n",
    "    #Best distance is used to ensure retrieval quality\n",
    "    if distances:\n",
    "        best_distance = min(distances)\n",
    "    else:\n",
    "        best_distance = 999\n",
    "    print(f\"Best distance is: {best_distance}\")\n",
    "\n",
    "\n",
    "\n",
    "    #If best distance is high (best chunks' relevance are low), quality is bad -> LLM fallback\n",
    "    if best_distance > maxDistance:\n",
    "        #Failed retrieval\n",
    "        print(\"Low similarity, now good matches found.\")\n",
    "        state[\"retrieval\"] = {\"chunks\": [], \"distances\": []}\n",
    "        state[\"retrieval_status\"] = \"failed\"\n",
    "        state[\"messages\"].append(AIMessage(content=\"No relevant context could be found.\"))\n",
    "        #Update state accordingly, which the LLM node will see\n",
    "        end = time.perf_counter()\n",
    "        print(f\"CONTROLLER node took {end-start:.1f} seconds\")\n",
    "        return state\n",
    "\n",
    "    #If keyword coverage is low in the retrieved chunks, more chunks are needed for better context\n",
    "    elif coverage < minCoverage:\n",
    "        #Retrieved but low coverage -> fetching more\n",
    "        print(\"\\nCoverage is low, retrying context retrieval with double the chunks...\")\n",
    "        query_vector = encodeQuery(query)\n",
    "        distances, indices = index.search(query_vector, kValue*2)\n",
    "        #Double the number of chunks retrieved - this can be adjusted freely\n",
    "        newChunks = [chunk_texts[i] for i in indices[0] if i<len(chunk_texts)]\n",
    "        state[\"retrieval\"] = {\n",
    "            \"chunks\": newChunks, \"distances\": distances[0].tolist()\n",
    "        }\n",
    "        #Retrieve the chunks and update state\n",
    "\n",
    "        newCoverage = keywordCoverage(query, newChunks)\n",
    "        if newCoverage < minCoverage:\n",
    "            #Still low coverage, reformulating question and fetching again\n",
    "            print(\"\\nNot enough coverage! Asking LLM to reformulate query.\")\n",
    "            refined = reformulateQuery(query)\n",
    "            print(f\"Refined query: {refined}\")\n",
    "            #Reformulate the query using the above function to potentially increase coverage\n",
    "\n",
    "            reformulatedCoverage = keywordCoverage(refined, newChunks)\n",
    "            if reformulatedCoverage < minCoverage:\n",
    "                #Low coverage even with reformulated question, failed retrieval\n",
    "                state[\"retrieval\"] = {\"chunks\": [], \"distances\": []}\n",
    "                state[\"retrieval_status\"] = \"failed\"\n",
    "                state[\"messages\"].append(AIMessage(content=\"No relevant context could be found.\"))\n",
    "                #Update state accordingly (fail)\n",
    "                end = time.perf_counter()\n",
    "                print(f\"CONTROLLER node took {end-start:.1f} seconds\")\n",
    "                return state\n",
    "            \n",
    "            #Reformulated question has good coverage, proceed with that\n",
    "            query_vector = encodeQuery(refined)\n",
    "            distances, indices = index.search(query_vector, kValue*2)\n",
    "            reformChunks = [chunk_texts[i] for i in indices[0] if i<len(chunk_texts)]\n",
    "            #Obtain relevant chunks\n",
    "            state[\"retrieval\"] = {\n",
    "                \"chunks\": reformChunks, \"distances\": distances[0].tolist()\n",
    "            }\n",
    "            state[\"messages\"].append(AIMessage(content=f\"Reformulated query used: {refined}\"))\n",
    "            state[\"retrieval_status\"] = \"success\"\n",
    "            #Update state accordingly (success)\n",
    "    \n",
    "    else:\n",
    "        #Enough coverage (success)\n",
    "        state[\"retrieval_status\"] = \"success\"\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(f\"CONTROLLER node took {end-start:.1f} seconds\")\n",
    "    return state\n",
    "graph_builder.add_node(\"controller\", controller_node)\n",
    "\n",
    "\n",
    "#The fourt node in the pipeline, responsible for combining fetched context with the query (RAG) and pass it on to the LLM\n",
    "def llm_node(state: State):\n",
    "    start = time.perf_counter()\n",
    "    query = state[\"messages\"][-1].content\n",
    "    chunks = state.get(\"retrieval\", {}).get(\"chunks\", [])\n",
    "    #Retrieve query and chunks from state \n",
    "    if not chunks:\n",
    "        answer = \"Could not find relevant information in the document(s). Please try rephrasing the query.\"\n",
    "        state[\"messages\"].append(AIMessage(content=answer))\n",
    "        #No relevant chunks found\n",
    "        end = time.perf_counter()\n",
    "        print(f\"LLM node took {end-start:.1f} seconds\")\n",
    "        return state\n",
    "    \n",
    "    #Else, join the chunks, and formulate a question to the LLM using the user question and the chunks\n",
    "    context = \"\\n\".join(chunks)\n",
    "    question = f\"Answer this question using the context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    #Construct headers and payload just like before, but with the question + chunks\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['API_KEY']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": langModel,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
    "    }\n",
    "    #Obtain response\n",
    "    resp = requests.post(SITE_URL, json=payload, headers=headers)\n",
    "    result = resp.json()\n",
    "\n",
    "    #Try to get answer\n",
    "    try:\n",
    "        answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except:\n",
    "        answer = f\"Error: {result}\"\n",
    "    #Add response message to state\n",
    "    state[\"messages\"].append(AIMessage(content=answer))\n",
    "    end = time.perf_counter()\n",
    "    print(f\"LLM node took {end-start:.1f} seconds\")\n",
    "    return state\n",
    "graph_builder.add_node(\"llm\", llm_node)\n",
    "\n",
    "\n",
    "#The \"last\" node in the pipeline (loops back to input), responsible for outputting the answer given by the LLM\n",
    "#Or if there are any error messages\n",
    "def output_node(state: State):\n",
    "    print(\"\\nAnswer: \")\n",
    "    print(state[\"messages\"][-1].content)\n",
    "    #Simply print out the message\n",
    "    return state\n",
    "graph_builder.add_node(\"output\", output_node)\n",
    "\n",
    "\n",
    "#Add edges showing the direction of the pipeline (state flow)\n",
    "graph_builder.add_edge(START, \"input\")\n",
    "graph_builder.add_edge(\"input\", \"retriever\")\n",
    "graph_builder.add_edge(\"retriever\", \"controller\")\n",
    "graph_builder.add_edge(\"controller\", \"llm\")\n",
    "graph_builder.add_edge(\"llm\", \"output\")\n",
    "graph_builder.add_edge(\"output\", \"input\")\n",
    "#From output, loop back to input to allow more chat with the user\n",
    "graph = graph_builder.compile()\n",
    "#Compile the graph and initialise the first state\n",
    "initial_state = State(messages=[])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        state = initial_state\n",
    "        while True:\n",
    "            state = graph.invoke(state)\n",
    "            #Run the graph\n",
    "    except KeyboardInterrupt:\n",
    "        #Handle user wanting to quit, gracefully\n",
    "        print(\"\\nUser pressed CTRL+C. Exiting gracefully...\")\n",
    "        exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
